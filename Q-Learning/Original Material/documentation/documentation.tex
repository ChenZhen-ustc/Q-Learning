\documentclass{IEEEtran}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[ruled,linesnumbered]{algorithm2e}

\begin{document}
\section{Environment}
\subsection{General parameters}
\begin{itemize}
    \item 100 by 100 meters squared area.
    \item 20 Users.
\end{itemize}
\subsection{User distribution}
\begin{itemize}
    \item The users are distributed around a local cluster.
    \item The center of the cluster is determined at random.
    \item It can take positions from 30 to 70, both in $x$ and $y$ coordinates.
    \item The users are scattered randomly with normal distribution around the cluster center, with mean $\mu = 0$ and standard deviation $\sigma = 20$.
    \item An example of the user distribution can be seen in Fig.~\ref{fig:users}.
\end{itemize}
\begin{figure}[h!]
    \resizebox{\columnwidth}{!}{\input{users.tex}}
    \caption{Example of user distribution in one training session.}\label{fig:users}
\end{figure}
\subsection{Drones}
\begin{itemize}
    \item The drones are assumed to have ideal communication among themselves.
    \item The drones share their positions and number of users allocated with each other.
    \item Each drone has a directional antenna with a main lobe with an aperture angle of $\theta=60$ degrees. The antenna is pointing downwards. An illustration of the directivity angle is presented in Fig.~\ref{fig:angle}
        \begin{figure}
            \centering
            \resizebox{.5\columnwidth}{!}{\input{droneAngle.tex}}
            \caption{Illustration of the coverage radius of a drone flying at $h_\text{d}$ meters and with directivity angle of $\theta$.}\label{fig:angle}
        \end{figure}
    \item There is no limit on the number of users that each drone can allocate.
    \item Each drone is assumed to be flying at a fixed height ${h_\text{d} = 30}$ meters.
\end{itemize}
\section{States}
\begin{itemize}
    \item The states are the positions of both drones.
    \item The environment is discretized into 121 possible positions for each drone (steps of 10 meters).
    \item Drones cannot assume the same position at the same time.
    \item The total number of states is therefore $2 \times \binom{121}{2} = 14520$.
\end{itemize}
\section{Actions}
\begin{itemize}
    \item The possible actions are to move $\pm 1$ step in $x$ or $y$.
    \item If one drone would move to the same position as the other (e.g\. chooses the action to move right when the other drone is one step to its right), it does not move.
    \item If a drone would move out of the grid (e.g\. chooses to move right when at $x$ coordinate 100), it does not move.
    \item If a state has not been explored, the action is also chosen at random, in order to avoid bias.
    \item Otherwise, actions are deterministic.
\end{itemize}
\section{Policy}
\begin{itemize}
    \item The policy is $\varepsilon$-greedy.
    \item Meaning that each drone chooses a random action with probability $\varepsilon$ and $\max (Q_{s_{t+1}, *})$ with probability $1 - \varepsilon$.
\end{itemize}
\section{Reward}
\begin{itemize}
    \item The reward is the total number of users allocated by both drones.
\end{itemize}
\subsection{User Association}
\begin{itemize}
    \item A user associates with a drone if its received SINR is above a threshold of 40 dB.
    \item The SINR for the link between user $u$ and drone $n$ is calculated according to
        \begin{equation}
            SINR_{n,u} = \frac{RSRP_{n, u}}{N + \sum_{\forall i \neq n} RSRP_{i, u}}.
        \end{equation}
    \item The RSRP for the link between user $u$ and drone $n$ is calculated according to the free space path loss, as
        \begin{equation}
            RSRP_{n,u} = \frac{P_\text{t}}{\frac{16 {(\pi  f_\text{c} d)}^2}{c^2}},
        \end{equation}
        where $c$ is the speed of light in meters per second, $P_\text{t}$ is the drone transmit power in Watts and $f_\text{c}$ is the carrier frequency in Hz.
    \item Any user outside the main lobe is considered to receive 0 W of power from the drone.
\end{itemize}
\section{Training}
\begin{itemize}
    \item The training session is comprised of 100 episodes of 20,000 iterations each.
    \item The state of the drones is set randomly at the beginning of each episode.
    \item $\varepsilon$ decays exponentially with the episode number, according to:
        \begin{equation}
            \varepsilon_j = \mathrm{e}^{-j/20},
        \end{equation}
        where $j$ is the episode number and $\mathrm{e}$ is Euler's constant.
\end{itemize}
\subsection{Algorithm}
The update strategy for SARSA is expressed as
\begin{equation}
    \begin{align}
        Q(s_t, a_t, \delta) = Q(s_t, a_t, \delta) &+ \\ \alpha (r_t + \gamma Q(s_{t+1},& a_{t+1}, \delta) - Q(s_t, a_t, \delta)),
    \end{align}
\end{equation}
where $\alpha$ is the learning rate and $\gamma$ is the discount factor.
A pseudo code of the implemented solution is presented in Algorithm~\ref{alg:SARSA}.
\begin{algorithm}[h]
    \SetKwFunction{random}{random}
    \SetKwFunction{chooseAction}{chooseAction}
    \SetKwFunction{takeAction}{takeAction}
    \SetKwFunction{computeReward}{computeReward}
    \SetKwFunction{updateQ}{updateQ}
        Initializations \\
    \For{Every episode $j$} {%
        $s_1$ \leftarrow~\random(1, 14520) \\
        \For{Every iteration $t$} {%
            \For{Every drone $\delta$} {%
                $a_t$ \leftarrow~\chooseAction($Q_{s_t,*}$, $\varepsilon_j$, $\delta$) \\
                $s_{t+1}$ \leftarrow~\takeAction($s_t$, $a_t$, $\delta$) \\
                $a_{t+1}$ \leftarrow~\chooseAction($Q_{s_{t+1},*}$, $\varepsilon_j$, $\delta$) \\
                $r_t$ \leftarrow~\computeReward($s_{t+1}$) \\
                $Q(s_t, a_t, d)$ \leftarrow~\updateQ()\\
                $s_t$ \leftarrow~$s_{t+1}$ \\
            }
        }
    }
\caption{SARSA implementation}\label{alg:SARSA}
\end{algorithm}
\section{Results}
The average reward per episode is shown in Fig.~\ref{fig:results}.
\begin{figure}[t!]
    \resizebox{\columnwidth}{!}{\input{reward.tex}}
    \caption{Average reward per episode considering for this training session}\label{fig:results}
\end{figure}
\end{document}


